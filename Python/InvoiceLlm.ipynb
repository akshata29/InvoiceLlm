{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Requirements\n",
    "\n",
    "* ~~Step 1 -> Extract the invoice header & line items for all invoices.~~\n",
    "  * A -> Add custom fields (like Loan#) that may be not part of standard fields that are extracted from Invoice Model\n",
    "  * B -> For duplicates, identify that as \"Duplicate\" and include all extracted metadata/attributes from that service request/invoice.\n",
    "  ~~* C -> Store the output in \"JSON\" (same format as excel spreadsheet as far as columns)~~ & SQL\n",
    "* Step 1a -> Use Layout Model and create the output in \"Markdown\" and persist that data and apply RAG pattern.\n",
    "* Step 1b -> Build custom model (potentially 2) to extract the data from Service Request/Invoice\n",
    "\n",
    "* ~~Step 2 -> Extract the metadata (at line level) from the \"Ground truth\" -> Excel/CSV file (Standard with \"Advances\" worksheet)~~\n",
    "\n",
    "* Step 3 -> Reconciliation Process\n",
    "  * A -> Reconcile Invoice line items (from Step 1) against Step 2 with various techniques (Fuzzy Description match across all duplicated invoice, ~~matching key metadata~~ - invoice date, payment date, ~~service date~~, ~~invoice#~~, amount, property address, description and vendor name) and based on the outcome, ~~generate the exception report for all line items not matching/missing from extracted data~~.  ~~Create the matching report as different file.~~\n",
    "  * B -> Reconcile Invoice line items (from Step 2) against Step 1 with various techniques (Fuzzy Description match across all duplicated invoice, ~~matching key metadata~~ - invoice date, payment date, ~~service date~~, ~~invoice#~~, amount, description and vendor name) and based on the outcome, ~~append the mismatch to existing exception report for all line items not matching from extracted data~~. (with indication of Step2 mismatch)\n",
    "\n",
    "* Step 3a -> LLM Reconciliation Process\n",
    "  * ~~Build a LLM Prompt such that it can compare JSON/SQL data identifying the match and exception based on key metadata called out earlier.~~\n",
    "               \n",
    "* Stretch Goal -> Build semantic model from the data that is stored in relation database & run PBI CoPilot on the top of that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import azure.functions as func\n",
    "import os\n",
    "import requests\n",
    "import urllib.parse\n",
    "from datetime import datetime, timedelta\n",
    "from azure.storage.blob import generate_container_sas\n",
    "from azure.identity import ManagedIdentityCredential, AzureCliCredential, ChainedTokenCredential\n",
    "import json\n",
    "import base64\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI, AzureOpenAI, AsyncAzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import json\n",
    "\n",
    "client = AzureOpenAI(\n",
    "                    api_key = os.getenv('OpenAiWestUsKey'),  \n",
    "                    api_version = os.getenv('OpenAiVersion'),\n",
    "                    azure_endpoint = os.getenv('OpenAiWestUsEp')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Convert our Ground truth Excel file into JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToJsonSerializable(data):  \n",
    "    if pd.isnull(data):  \n",
    "        # Convert NaN or NaT to None  \n",
    "        return None  \n",
    "    elif isinstance(data, pd.Timestamp):  \n",
    "        # Convert Timestamp to ISO 8601 format string  \n",
    "        return data.date().isoformat()\n",
    "    else:  \n",
    "        # Return other data types unchanged  \n",
    "        return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanNumber = \"4000835968\"\n",
    "dataDirectory = \"../Data/Loan/\" + loanNumber + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file has been created: ../Data/Loan/4000835968//Python/4000835968.json\n"
     ]
    }
   ],
   "source": [
    "excelFile = dataDirectory + loanNumber + \".xlsx\"\n",
    "outputJson = dataDirectory + \"/Python/\" + loanNumber + \".json\"\n",
    "\n",
    "# Load the Excel file  \n",
    "xlsx = pd.ExcelFile(excelFile)  \n",
    "  \n",
    "# Create a dictionary to store the data from each sheet  \n",
    "data = {}  \n",
    "  \n",
    "# Iterate through each worksheet in the Excel file  \n",
    "for sheetName in xlsx.sheet_names:\n",
    "    if sheetName != \"Advances\" and sheetName != \"Summary\":\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_excel(xlsx, sheetName)\n",
    "\n",
    "    # Apply the conversion function to each cell in the DataFrame  \n",
    "    df = df.applymap(convertToJsonSerializable)\n",
    "      \n",
    "    # Convert the DataFrame to a dictionary\n",
    "    if sheetName == \"Advances\":\n",
    "        data[sheetName] = df.to_dict(orient='records')\n",
    "    #else:\n",
    "    #    data[sheetName] = df.to_dict(orient='records')\n",
    "\n",
    "# Convert the dictionary to a JSON string \n",
    "json_data = json.dumps(data, indent=4, ensure_ascii=False)  \n",
    "  \n",
    "# Optionally, you can save this JSON data to a file  \n",
    "with open(outputJson, 'w') as json_file:  \n",
    "    json_file.write(json_data)  \n",
    "  \n",
    "print(f'JSON file has been created: {outputJson}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv  \n",
    "# import json  \n",
    "  \n",
    "# invoiceSample = \"./Data/Invoice/0084518695.csv\"\n",
    "  \n",
    "# # Replace 'output.json' with the desired path for the JSON output file  \n",
    "# outputJson = './Data/Invoice/0084518695.json'  \n",
    "  \n",
    "# # Read the CSV and convert it to a dictionary  \n",
    "# data = []  \n",
    "# with open(invoiceSample, mode='r', encoding='utf-8') as csvFile:  \n",
    "#     reader = csv.DictReader(csvFile)  \n",
    "#     for row in reader:  \n",
    "#         if any(field.strip() for field in row.values()):  # Check for non-blank rows  \n",
    "#             data.append(row)\n",
    "  \n",
    "# # Write the dictionary to a JSON file  \n",
    "# with open(outputJson, mode='w', encoding='utf-8') as jsonFile:  \n",
    "#     json.dump(data, jsonFile, indent=4)  \n",
    "  \n",
    "# print(f'JSON file has been created: {outputJson}')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Invoke the Document Intelligence - Invoice Pre-built Model including the Key-Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from requests import get, post\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import re\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceJsonPlaceHolders(json, values):\n",
    "  # find all placeholders\n",
    "  placeholders = re.findall('<[\\w ]+>', json)\n",
    "  assert len(placeholders) == len(values), \"Please enter the values of all placeholders.\"\n",
    "\n",
    "  # replaces all placeholders with values\n",
    "  for k, v in values.items():\n",
    "      placeholder = \"<%s>\" % k\n",
    "      json = json.replace(placeholder, v)\n",
    "\n",
    "  return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceJsonPlaceHolder(json, values):\n",
    "  # replaces all placeholders with values\n",
    "  for k, v in values.items():\n",
    "      placeholder = \"<%s>\" % k\n",
    "      json = json.replace(placeholder, str(v))\n",
    "\n",
    "  return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime.date):\n",
    "            return obj.isoformat()\n",
    "        return super().default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDocs = []\n",
    "# sampleDocs = [\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_451102775.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_451102776.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_458680339.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_458680340.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_462460604.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_462460605.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_467091874.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_467091875.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_467091876.pdf',\n",
    "# ]\n",
    "# sampleDocs = [\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_451102775.pdf',\n",
    "# ]\n",
    "sampleOutputDocs = []\n",
    "sourcePath = dataDirectory + \"CORPADV/\"\n",
    "destinationPath = dataDirectory + \"Python/\"\n",
    "for file in os.listdir(sourcePath):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        sampleDocs.append(sourcePath + file)\n",
    "\n",
    "for file in os.listdir(destinationPath):\n",
    "    if file.endswith(\".json\"):\n",
    "        sampleOutputDocs.append(destinationPath + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractedData = []\n",
    "templateStructure = '{\"Payment Date\":\"<Payment_Date>\",\"Service Date\":\"<Service_Date>\", \"Expense Type\": \"\",\"Additional Expense Comments\":\"<Additional_Expense_Comments>\",\"Expense Description\": \"<Expense_Description>\",\"Amount Paid\": \"<Amount_Paid>\",\"Amount Claimed\": \"\",\"Amount Not Claimed\": \"\",\"Unclaimed Amount Reason\": \"\",\"Vendor Name\": \"<Vendor_Name>\",\"Invoice Number\": \"<Invoice_Number>\",\"Fee Type Code\": \"\",\"Recovery Type\": \"\",\"Actual Recovery Code\": \"\",\"Expense Code\": \"\",\"Fee Reference Comments\": \"\",\"File Name\": \"\",\"Document Available\": \"\",\"Notes\": \"<Notes>\"}'\n",
    "docIntelligenceEndPoint = os.getenv('FormRecognizerEndPoint')\n",
    "docIntelligenceKey = os.getenv('FormRecognizerKey')\n",
    "docAnalysisClient = DocumentAnalysisClient(\n",
    "        endpoint=docIntelligenceEndPoint, credential=AzureKeyCredential(docIntelligenceKey))\n",
    "\n",
    "output = {\"Advances\": []}\n",
    "for sampleDoc in sampleDocs:\n",
    "    fileName = os.path.basename(sampleDoc).replace(\".pdf\", \".json\")\n",
    "    # Check if we already have ran the analysis\n",
    "    #if os.path.exists(sampleDoc.replace(\".pdf\", \".json\")):\n",
    "    if os.path.exists(destinationPath + fileName):\n",
    "        print(\"--------Process Already analyzed Invoice: \", sampleDoc)\n",
    "        #with open(sampleDoc.replace(\".pdf\", \".json\"), \"r\") as f:\n",
    "        with open(destinationPath + fileName, \"r\") as f:\n",
    "            invoices = json.load(f)\n",
    "        paymentDate = ''\n",
    "        serviceDate = ''\n",
    "        invoiceNbr = ''\n",
    "        vendorName = ''\n",
    "        expenseDesc = ''\n",
    "        additionalExpenseComments = ''\n",
    "        amountPaid = ''\n",
    "        notes = ''\n",
    "        for idx, invoice in enumerate(invoices[\"documents\"]):\n",
    "            for name, field in invoice[\"fields\"].items():\n",
    "                if name != \"Items\":\n",
    "                    if name == \"VendorName\":\n",
    "                        vendorName = field[\"content\"]\n",
    "                    if name == \"InvoiceId\":\n",
    "                        invoiceNbr = field[\"content\"]\n",
    "                    if name == \"CustomerAddress\":\n",
    "                        notes = field[\"content\"]\n",
    "\n",
    "                    #print(\"...{}: {} has confidence {}\".format(name, field.content, field.confidence))\n",
    "\n",
    "            for idx, item in enumerate(invoice[\"fields\"].get(\"Items\").get(\"value\")):\n",
    "                print(\"...Item #{}\".format(idx))\n",
    "                for name, field in item[\"value\"].items():\n",
    "                    if name == \"Amount\" and field[\"value_type\"] == \"currency\":\n",
    "                        try:\n",
    "                            amountPaid = field[\"value\"][\"amount\"]\n",
    "                        except:\n",
    "                            amountPaid = field[\"content\"]\n",
    "                    if name == \"Date\" and field[\"value_type\"] == \"date\":\n",
    "                        try:\n",
    "                            serviceDate = field[\"value\"]\n",
    "                        except:\n",
    "                            serviceDate = field[\"content\"]\n",
    "                        print(serviceDate)\n",
    "                    if name == \"Description\":\n",
    "                        expenseDesc = field[\"content\"]\n",
    "\n",
    "                    #print(\"......{}: {} has confidence {}\".format(name, field.content, field.confidence))\n",
    "\n",
    "        for idx, kv in enumerate(invoices[\"key_value_pairs\"]):\n",
    "            if (kv[\"value\"] != None):\n",
    "                if (kv[\"key\"][\"content\"] and kv[\"value\"][\"content\"]):\n",
    "                    if kv[\"key\"][\"content\"] == \"PaymentDate\":\n",
    "                        paymentDate = kv[\"value\"][\"content\"]\n",
    "                    #print(\"Key...{}: Value...{}\".format(kv.key.content, kv.value.content))\n",
    "        \n",
    "        values = {'Service_Date':serviceDate,'Amount_Paid': amountPaid, 'Vendor_Name': vendorName, \n",
    "                  'Invoice_Number': invoiceNbr.removeprefix(\"60\"), 'Expense_Description': expenseDesc, 'Additional_Expense_Comments': additionalExpenseComments, 'Notes': notes}\n",
    "        print(values)\n",
    "        filledItem = replaceJsonPlaceHolder(templateStructure,values)\n",
    "        extractedData.append(ast.literal_eval(json.dumps(filledItem)))\n",
    "        continue\n",
    "    else:\n",
    "        print(\"--------Process Invoice: \", sampleDoc)\n",
    "        with open(sampleDoc, \"rb\") as f:\n",
    "            poller = docAnalysisClient.begin_analyze_document(\n",
    "                \"prebuilt-invoice\", document=f, locale=\"en-US\",\n",
    "                pages=\"1\", features=[\"keyValuePairs\"]\n",
    "            )\n",
    "        invoices = poller.result()\n",
    "        with open(destinationPath + fileName, \"w\") as f:\n",
    "            json.dump(invoices.to_dict(), f, indent=4, default=str)\n",
    "        paymentDate = ''\n",
    "        serviceDate = ''\n",
    "        invoiceNbr = ''\n",
    "        vendorName = ''\n",
    "        expenseDesc = ''\n",
    "        additionalExpenseComments = ''\n",
    "        amountPaid = ''\n",
    "        notes = ''\n",
    "        for idx, invoice in enumerate(invoices.documents):\n",
    "            for name, field in invoice.fields.items():\n",
    "                if name != \"Items\":\n",
    "                    if name == \"VendorName\":\n",
    "                        vendorName = field.content\n",
    "                    if name == \"InvoiceId\":\n",
    "                        invoiceNbr = field.content\n",
    "                    if name == \"CustomerAddress\":\n",
    "                        notes = field.content\n",
    "\n",
    "                    #print(\"...{}: {} has confidence {}\".format(name, field.content, field.confidence))\n",
    "\n",
    "            for idx, item in enumerate(invoice.fields.get(\"Items\").value):\n",
    "                #print(\"...Item #{}\".format(idx))\n",
    "                for name, field in item.value.items():\n",
    "                    if name == \"Amount\" and field.value_type == \"currency\":\n",
    "                        try:\n",
    "                            amountPaid = field.value.amount\n",
    "                        except:\n",
    "                            amountPaid = field.content\n",
    "                    if name == \"Date\" and field.value_type == \"date\":\n",
    "                        try:\n",
    "                            serviceDate = field.value\n",
    "                        except:\n",
    "                            serviceDate = field.content\n",
    "                    if name == \"Description\":\n",
    "                        expenseDesc = field.content\n",
    "\n",
    "                    #print(\"......{}: {} has confidence {}\".format(name, field.content, field.confidence))\n",
    "\n",
    "        for idx, kv in enumerate(invoices.key_value_pairs):\n",
    "            if (kv.value != None):\n",
    "                if (kv.key.content and kv.value.content):\n",
    "                    if kv.key.content == \"PaymentDate\":\n",
    "                        paymentDate = kv.value.content\n",
    "                    #print(\"Key...{}: Value...{}\".format(kv.key.content, kv.value.content))\n",
    "        \n",
    "        values = {'Service_Date':serviceDate,'Amount_Paid': amountPaid, 'Vendor_Name': vendorName, \n",
    "                'Invoice_Number': invoiceNbr.removeprefix(\"60\"), 'Expense_Description': expenseDesc, 'Additional_Expense_Comments': additionalExpenseComments, 'Notes': notes}\n",
    "        print(values)\n",
    "        filledItem = replaceJsonPlaceHolder(templateStructure,values)\n",
    "        extractedData.append(ast.literal_eval(json.dumps(filledItem)))\n",
    "\n",
    "output[\"Advances\"] = extractedData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedOutput = json.dumps(output, indent=4, ensure_ascii=False)\n",
    "updatedOutput = updatedOutput.replace(\"\\\"{\", \"{\")\n",
    "updatedOutput = updatedOutput.replace(\"}\\\"\", \"}\")\n",
    "updatedOutput = updatedOutput.replace(\"\\\\\", \"\")\n",
    "processedOutputJson = dataDirectory + \"Python/\" + loanNumber + \"_FrOut.json\"\n",
    "# Optionally, you can save this JSON data to a file  \n",
    "with open(processedOutputJson, 'w') as json_file:  \n",
    "    json_file.write(updatedOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3A - Compare the Ground Truth JSON with the Document Intelligence JSON Output - Using \"Python\" Code\n",
    "##### Ensure that the output JSON is in the same format as the Ground Truth JSON for both Matching and Non-Matching Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and parse a JSON file  \n",
    "def loadJson(file_path):\n",
    "    with open(file_path, 'r') as file:  \n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare two lists of dictionaries based on key fields  \n",
    "def compareJsonArray(jsonArray1, jsonArray2, keyFields):  \n",
    "    matchingOutput = {\"Advances\": []}\n",
    "    nonMatchingOutput = {\"Advances\": []}\n",
    "    matchingObjects = []  \n",
    "    nonMatchingObjects = []  \n",
    "  \n",
    "    # Convert the second JSON array to a dictionary for faster lookup  \n",
    "    json_dict2 = {tuple(item[key] for key in keyFields): item for item in jsonArray2}  \n",
    "  \n",
    "    # Iterate through the first JSON array and compare  \n",
    "    for item1 in jsonArray1:  \n",
    "        key = tuple(item1[key] for key in keyFields)  \n",
    "        item2 = json_dict2.get(key)  \n",
    "        if item2:  \n",
    "            # If a matching object is found based on key fields, store it  \n",
    "            #matchingObjects.append({'object1': item1, 'object2': item2})\n",
    "            matchingObjects.append(item1)\n",
    "        else:  \n",
    "            # If no matching object is found, store the non-matching object from the first array  \n",
    "            nonMatchingObjects.append(item1)  \n",
    "  \n",
    "    # Also check for any objects in the second array that didn't match any in the first  \n",
    "    for item2 in jsonArray2:  \n",
    "        key = tuple(item2[key] for key in keyFields)  \n",
    "        if key not in json_dict2:  \n",
    "            nonMatchingObjects.append(item2)  \n",
    "  \n",
    "    matchingOutput[\"Advances\"] = matchingObjects\n",
    "    nonMatchingOutput[\"Advances\"] = nonMatchingObjects\n",
    "    return matchingOutput, nonMatchingOutput  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonArray1 = loadJson(dataDirectory + \"Python/\" + loanNumber + \".json\")\n",
    "jsonArray2 = loadJson(dataDirectory + \"Python/\" + loanNumber + \"_FrOut.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching, nonMatching = compareJsonArray(jsonArray1['Advances'], jsonArray2['Advances'], \n",
    "                 ['Invoice Number', \"Service Date\"])\n",
    "# Output the results  \n",
    "#print(\"Matching Objects:\")  \n",
    "#print(json.dumps(matching, indent=4))  \n",
    "#print(\"\\nNon-Matching Objects:\")  \n",
    "#print(json.dumps(nonMatching, indent=4))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Incase if required to find duplicates\n",
    "\n",
    "# import hashlib\n",
    "\n",
    "# matchingFiltered = []\n",
    "# md5List = []\n",
    "\n",
    "# for item in matching[\"Advances\"]:\n",
    "#     md5Result = hashlib.md5(json.dumps(item, separators=(',', ':')).encode(\"utf-8\")).hexdigest()\n",
    "#     if md5Result not in md5List:\n",
    "#         md5List.append(md5Result)\n",
    "#         matchingFiltered.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchingOutputJson = dataDirectory + \"Python/\" + loanNumber + \"_FrMatching.json\"\n",
    "nonMatchingOutputJson = dataDirectory + \"Python/\" + loanNumber + \"_FrNonMatching.json\"\n",
    "# Optionally, you can save this JSON data to a file  \n",
    "with open(matchingOutputJson, 'w') as json_file:\n",
    "    json_file.write(json.dumps(matching, indent=4))\n",
    "\n",
    "with open(nonMatchingOutputJson, 'w') as json_file:\n",
    "    json_file.write(json.dumps(nonMatching, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3b - Invoke LLM and build Custom Prompt that can be used to Compare the Ground Truth JSON with the Document Intelligence JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncateToken(string: str, encoding_name: str, max_length: int = 128000) -> str:\n",
    "    \"\"\"Truncates a text string based on max number of tokens.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    num_tokens = len(encoded_string)\n",
    "\n",
    "    if num_tokens > max_length:\n",
    "        string = encoding.decode(encoded_string[:max_length])\n",
    "\n",
    "    return string\n",
    "\n",
    "def getMessagesFromHistory(systemPrompt: str, userConv: str):\n",
    "        #messageBuilder = MessageBuilder(systemPrompt, modelId)\n",
    "        messages = []\n",
    "        messages.append({'role': 'system', 'content': systemPrompt})\n",
    "        userContent = truncateToken(string=userConv, encoding_name=\"cl100k_base\", max_length=128000)\n",
    "        messages.append({'role': \"user\", 'content': userContent})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# systemTemplate = \"\"\"Compare the provided JSON data arrays and determine the matching and non-matching rows based on key fields. Display the JSON output for both the matching and non-matching objects.\n",
    "#                     Context:\n",
    "#                     You have been given two JSON data arrays, Array A and Array B. Your task is to compare these arrays and identify the rows that match and do not match based on key fields. \n",
    "#                     The key fields in the JSON data arrays are \"Invoice Number\", \"Service Date\".\n",
    "                    \n",
    "#                     Instructions:\n",
    "#                     Compare Array A and Array B based on specified key fields.\n",
    "#                     Identify and display the matching rows in JSON format.\n",
    "#                     Identify and display the non-matching rows in JSON format.\n",
    "#                     Ensure that the output clearly indicates whether a row is matching or non-matching.\n",
    "                    \n",
    "#                     Do not include any explanations, only provide a  RFC8259 compliant JSON response following this format without deviation.\n",
    "#                     [{\n",
    "#                         \"Payment Date\": \"Payment_Date\",\n",
    "#                         \"Service Date\": \"2022-02-10\",\n",
    "#                         \"Expense Type\": \"Expense_Type\",\n",
    "#                         \"Additional Expense Comments\": \"Comments here\",\n",
    "#                         \"Expense Description\": \"Description here\",\n",
    "#                         \"Amount Paid\": \"Amount here\",\n",
    "#                         \"Amount Claimed\": \"Amount Claimed here\",\n",
    "#                         \"Amount Not Claimed\": \"Amount Not Claimed here\",\n",
    "#                         \"Unclaimed Amount Reason\": \"\",\n",
    "#                         \"Vendor Name\": \"Vendor Name here\",\n",
    "#                         \"Invoice Number\": \"Invoice Number here\",\n",
    "#                         \"Fee Type Code\": \"\",\n",
    "#                         \"Recovery Type\": \"\",\n",
    "#                         \"Actual Recovery Code\": \"\",\n",
    "#                         \"Expense Code\": \"\",\n",
    "#                         \"Fee Reference Comments\": \"\",\n",
    "#                         \"File Name\": \"\",\n",
    "#                         \"Document Available\": \"\",\n",
    "#                         \"Notes\": \"Notes here\"\n",
    "#                         }]\n",
    "#                         \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the LLM to compare two JSON data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemTemplate = \"\"\"Compare the provided JSON data arrays and determine the matching rows based on key fields. Display the JSON output for the matching objects.\n",
    "                    Context:\n",
    "                    You have been given two JSON data arrays, Array A and Array B. Your task is to compare these arrays and identify the rows that match and do not match based on key fields. \n",
    "                    The key fields in the JSON data arrays are \"Invoice Number\", \"Service Date\".\n",
    "                    \n",
    "                    Instructions:\n",
    "                    Compare Array A and Array B based on specified key fields.\n",
    "                    Identify and display the matching rows in JSON format.\n",
    "                    \n",
    "                    Do not include any explanations, only provide a  RFC8259 compliant JSON response following this format without deviation.\n",
    "                    {\n",
    "                        \"Advances\":[{\n",
    "                            \"Payment Date\": \"Payment_Date\",\n",
    "                            \"Service Date\": \"2022-02-10\",\n",
    "                            \"Expense Type\": \"Expense_Type\",\n",
    "                            \"Additional Expense Comments\": \"Comments here\",\n",
    "                            \"Expense Description\": \"Description here\",\n",
    "                            \"Amount Paid\": \"Amount here\",\n",
    "                            \"Amount Claimed\": \"Amount Claimed here\",\n",
    "                            \"Amount Not Claimed\": \"Amount Not Claimed here\",\n",
    "                            \"Unclaimed Amount Reason\": \"\",\n",
    "                            \"Vendor Name\": \"Vendor Name here\",\n",
    "                            \"Invoice Number\": \"Invoice Number here\",\n",
    "                            \"Fee Type Code\": \"\",\n",
    "                            \"Recovery Type\": \"\",\n",
    "                            \"Actual Recovery Code\": \"\",\n",
    "                            \"Expense Code\": \"\",\n",
    "                            \"Fee Reference Comments\": \"\",\n",
    "                            \"File Name\": \"\",\n",
    "                            \"Document Available\": \"\",\n",
    "                            \"Notes\": \"Notes here\"\n",
    "                        }]\n",
    "                    }\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because of the limitation on 4096 Tokens on the completion, let's break down the steps into multiple chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = (len(jsonArray1['Advances']) - 1) // 5 + 1\n",
    "for i in range(chunks):\n",
    "     processedOutputJson = dataDirectory + \"Python/\" + loanNumber + \"_LlmMatching\" + str(i) + \".json\"\n",
    "     if (os.path.exists(processedOutputJson)):\n",
    "          continue\n",
    "     chunkedJsonArray1 = {\"Advances\": []}\n",
    "     batch = jsonArray1['Advances'][i*5:(i+1)*5]\n",
    "     chunkedJsonArray1[\"Advances\"] = batch\n",
    "     #print(chunkedJsonArray1)\n",
    "     content = \"\"\"Array A:{arrayA}\n",
    "            Array B:{arrayB}\n",
    "            \"\"\"\n",
    "     content = content.format(arrayA=json.dumps(chunkedJsonArray1, indent=4), arrayB=json.dumps(jsonArray2, indent=4))\n",
    "\n",
    "     messages = []\n",
    "     messages.append({'role': 'system', 'content': systemTemplate})\n",
    "     #userContent = truncateToken(string=content, encoding_name=\"gpt-4-1106-preview\", max_length=75000)\n",
    "     userContent = content\n",
    "     messages.append({'role': \"user\", 'content': userContent})\n",
    "\n",
    "     completion = client.chat.completions.create(\n",
    "          model=os.getenv('OpenAiGpt4Turbo'), \n",
    "          messages=messages,\n",
    "          temperature=0,\n",
    "          top_p=0,\n",
    "          max_tokens=4096,\n",
    "          n=1)\n",
    "     answer = completion.choices[0].message.content\n",
    "     answer = re.sub('\\n', '', answer)\n",
    "     answer = re.sub('```json', '', answer)\n",
    "     answer = re.sub('```', '', answer)\n",
    "\n",
    "     llmAnswer = json.loads(answer)\n",
    "     # Optionally, you can save this JSON data to a file  \n",
    "     with open(processedOutputJson, 'w') as json_file:  \n",
    "          json_file.write(json.dumps(llmAnswer, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all our LlmMatching Outputs to a single file\n",
    "llmMatchingOutput = {\"Advances\": []}\n",
    "for i in range(chunks):\n",
    "     with open(dataDirectory + \"Python/\" + loanNumber + \"_LlmMatching\" + str(i) + \".json\", \"r\") as f:\n",
    "          llmMatchingOutput[\"Advances\"].extend(json.load(f)[\"Advances\"])\n",
    "\n",
    "processedOutputJson = dataDirectory + \"Python/\" + loanNumber + \"_LlmMatching.json\"\n",
    "# Optionally, you can save this JSON data to a file  \n",
    "with open(processedOutputJson, 'w') as json_file:  \n",
    "    json_file.write(json.dumps(llmMatchingOutput, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run LLM to find non-matching rows between two JSON data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemTemplate = \"\"\"Compare the provided JSON data arrays and determine the non-matching rows based on key fields. Display the JSON output for the non-matching objects.\n",
    "                    Context:\n",
    "                    You have been given two JSON data arrays, Array A and Array B. Your task is to compare these arrays and identify the rows that do not match and do not match based on key fields. \n",
    "                    The key fields in the JSON data arrays are \"Invoice Number\", \"Service Date\".\n",
    "                    \n",
    "                    Instructions:\n",
    "                    Compare Array A and Array B based on specified key fields.\n",
    "                    Identify and display the non-matching rows in JSON format.\n",
    "                    \n",
    "                    Do not include any explanations, only provide a  RFC8259 compliant JSON response following this format without deviation.\n",
    "                    {\n",
    "                        \"Advances\":[{\n",
    "                            \"Payment Date\": \"Payment_Date\",\n",
    "                            \"Service Date\": \"2022-02-10\",\n",
    "                            \"Expense Type\": \"Expense_Type\",\n",
    "                            \"Additional Expense Comments\": \"Comments here\",\n",
    "                            \"Expense Description\": \"Description here\",\n",
    "                            \"Amount Paid\": \"Amount here\",\n",
    "                            \"Amount Claimed\": \"Amount Claimed here\",\n",
    "                            \"Amount Not Claimed\": \"Amount Not Claimed here\",\n",
    "                            \"Unclaimed Amount Reason\": \"\",\n",
    "                            \"Vendor Name\": \"Vendor Name here\",\n",
    "                            \"Invoice Number\": \"Invoice Number here\",\n",
    "                            \"Fee Type Code\": \"\",\n",
    "                            \"Recovery Type\": \"\",\n",
    "                            \"Actual Recovery Code\": \"\",\n",
    "                            \"Expense Code\": \"\",\n",
    "                            \"Fee Reference Comments\": \"\",\n",
    "                            \"File Name\": \"\",\n",
    "                            \"Document Available\": \"\",\n",
    "                            \"Notes\": \"Notes here\"\n",
    "                        }]\n",
    "                    }\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Excel Data:  63\n",
      "Total FrData:  52\n",
      "Total FrMatching:  23\n",
      "Total FrNoMatching:  40\n",
      "Total LlmMatching:  29\n"
     ]
    }
   ],
   "source": [
    "excelData = dataDirectory + \"Python/\" + loanNumber + \".json\"\n",
    "FrData = dataDirectory + \"Python/\" + loanNumber + \"_FrOut.json\"\n",
    "FrMatching = dataDirectory + \"Python/\" + loanNumber + \"_FrMatching.json\"\n",
    "FrNoMatching = dataDirectory + \"Python/\" + loanNumber + \"_FrNonMatching.json\"\n",
    "LlmMatching = dataDirectory + \"Python/\" + loanNumber + \"_LlmMatching.json\"\n",
    "LlmNoMatching = dataDirectory + \"Python/\" + loanNumber + \"_LlmNonMatching.json\"\n",
    "\n",
    "with open(excelData, 'r') as file:\n",
    "    excelData = json.load(file)\n",
    "    print(\"Total Excel Data: \", len(excelData['Advances']))\n",
    "\n",
    "with open(FrData, 'r') as file:\n",
    "    FrData = json.load(file)\n",
    "    print(\"Total FrData: \", len(FrData['Advances']))\n",
    "\n",
    "with open(FrMatching, 'r') as file:\n",
    "    FrMatching = json.load(file)\n",
    "    print(\"Total FrMatching: \", len(FrMatching['Advances']))\n",
    "\n",
    "with open(FrNoMatching, 'r') as file:\n",
    "    FrNoMatching = json.load(file)\n",
    "    print(\"Total FrNoMatching: \", len(FrNoMatching['Advances']))\n",
    "\n",
    "with open(LlmMatching, 'r') as file:\n",
    "    LlmMatching = json.load(file)\n",
    "    print(\"Total LlmMatching: \", len(LlmMatching['Advances']))\n",
    "\n",
    "# with open(LlmNoMatching, 'r') as file:\n",
    "#     LlmNoMatching = json.load(file)\n",
    "#     print(\"Total LlmNoMatching: \", len(LlmNoMatching['Advances']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
