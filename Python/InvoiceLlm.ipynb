{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Requirements\n",
    "\n",
    "* ~~Step 1 -> Extract the invoice header & line items for all invoices.~~\n",
    "  * A -> Add custom fields (like Loan#) that may be not part of standard fields that are extracted from Invoice Model\n",
    "  * B -> For duplicates, identify that as \"Duplicate\" and include all extracted metadata/attributes from that service request/invoice.\n",
    "  ~~* C -> Store the output in \"JSON\" (same format as excel spreadsheet as far as columns)~~ & SQL\n",
    "* Step 1a -> Use Layout Model and create the output in \"Markdown\" and persist that data and apply RAG pattern.\n",
    "* Step 1b -> Build custom model (potentially 2) to extract the data from Service Request/Invoice\n",
    "\n",
    "* ~~Step 2 -> Extract the metadata (at line level) from the \"Ground truth\" -> Excel/CSV file (Standard with \"Advances\" worksheet)~~\n",
    "\n",
    "* Step 3 -> Reconciliation Process\n",
    "  * A -> Reconcile Invoice line items (from Step 1) against Step 2 with various techniques (Fuzzy Description match across all duplicated invoice, ~~matching key metadata~~ - invoice date, payment date, ~~service date~~, ~~invoice#~~, amount, property address, description and vendor name) and based on the outcome, ~~generate the exception report for all line items not matching/missing from extracted data~~.  ~~Create the matching report as different file.~~\n",
    "  * B -> Reconcile Invoice line items (from Step 2) against Step 1 with various techniques (Fuzzy Description match across all duplicated invoice, ~~matching key metadata~~ - invoice date, payment date, ~~service date~~, ~~invoice#~~, amount, description and vendor name) and based on the outcome, ~~append the mismatch to existing exception report for all line items not matching from extracted data~~. (with indication of Step2 mismatch)\n",
    "\n",
    "* Step 3a -> LLM Reconciliation Process\n",
    "  * ~~Build a LLM Prompt such that it can compare JSON/SQL data identifying the match and exception based on key metadata called out earlier.~~\n",
    "               \n",
    "* Stretch Goal -> Build semantic model from the data that is stored in relation database & run PBI CoPilot on the top of that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import azure.functions as func\n",
    "import os\n",
    "import requests\n",
    "import urllib.parse\n",
    "from datetime import datetime, timedelta\n",
    "from azure.storage.blob import generate_container_sas\n",
    "from azure.identity import ManagedIdentityCredential, AzureCliCredential, ChainedTokenCredential\n",
    "import json\n",
    "import base64\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI, AzureOpenAI, AsyncAzureOpenAI\n",
    "import pandas as pd  \n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "                    api_key = os.getenv('OpenAiWestUsKey'),  \n",
    "                    api_version = os.getenv('OpenAiVersion'),\n",
    "                    azure_endpoint = os.getenv('OpenAiWestUsEp')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Convert our Ground truth Excel file into JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToJsonSerializable(data):  \n",
    "    if pd.isnull(data):  \n",
    "        # Convert NaN or NaT to None  \n",
    "        return None  \n",
    "    elif isinstance(data, pd.Timestamp):  \n",
    "        # Convert Timestamp to ISO 8601 format string  \n",
    "        return data.date().isoformat()\n",
    "    else:  \n",
    "        # Return other data types unchanged  \n",
    "        return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-req from previous process - Need the Loan# (as folder) and excel File as same name as Loan# in the folder\n",
    "#### Within the folder, all sub-folders with Invoice/PDF will be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanNumber = \"4000835968\"\n",
    "dataDirectory = \"../Data/Loan/\" + loanNumber + \"/\"\n",
    "CorpAdv = \"CORPADV/\"\n",
    "Invoice = \"INVOICE/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file has been created: ../Data/Loan/4000835968//Python/4000835968.json\n"
     ]
    }
   ],
   "source": [
    "excelFile = dataDirectory + loanNumber + \".xlsx\"\n",
    "outputJson = dataDirectory + \"/Python/\" + loanNumber + \".json\"\n",
    "\n",
    "# Load the Excel file  \n",
    "xlsx = pd.ExcelFile(excelFile)  \n",
    "  \n",
    "# Create a dictionary to store the data from each sheet  \n",
    "data = {}  \n",
    "  \n",
    "# Iterate through each worksheet in the Excel file  \n",
    "for sheetName in xlsx.sheet_names:\n",
    "    if sheetName != \"Advances\" and sheetName != \"Summary\":\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_excel(xlsx, sheetName)\n",
    "\n",
    "    # Apply the conversion function to each cell in the DataFrame  \n",
    "    df = df.applymap(convertToJsonSerializable)\n",
    "\n",
    "    # Convert the DataFrame to a dictionary\n",
    "    if sheetName == \"Advances\":\n",
    "        df = df.rename(columns={'Payment Date': 'Payment_Date', 'Service Date': 'Service_Date', 'Invoice Number': 'Invoice_Nbr', \n",
    "                        'Expense Description': 'Item_Description', 'Amount Paid': 'Item_Amount'})\n",
    "        subsetDf = df[['Payment_Date', 'Service_Date', 'Invoice_Nbr', 'Item_Description', 'Item_Amount']]\n",
    "        filteredDf = subsetDf[subsetDf[\"Item_Amount\"] > 0]\n",
    "        filteredDf = filteredDf.reset_index()\n",
    "        filteredDf = filteredDf.rename(columns={\"index\":\"Row_Id\"})\n",
    "        data = filteredDf.to_dict(orient='records')\n",
    "    #else:\n",
    "    #    data[sheetName] = df.to_dict(orient='records')\n",
    "\n",
    "# Convert the dictionary to a JSON string \n",
    "json_data = json.dumps(data, indent=4, ensure_ascii=False)\n",
    "  \n",
    "# Optionally, you can save this JSON data to a file  \n",
    "with open(outputJson, 'w') as json_file:  \n",
    "    json_file.write(json_data)  \n",
    "  \n",
    "print(f'JSON file has been created: {outputJson}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv  \n",
    "# import json  \n",
    "  \n",
    "# invoiceSample = \"./Data/Invoice/0084518695.csv\"\n",
    "  \n",
    "# # Replace 'output.json' with the desired path for the JSON output file  \n",
    "# outputJson = './Data/Invoice/0084518695.json'  \n",
    "  \n",
    "# # Read the CSV and convert it to a dictionary  \n",
    "# data = []  \n",
    "# with open(invoiceSample, mode='r', encoding='utf-8') as csvFile:  \n",
    "#     reader = csv.DictReader(csvFile)  \n",
    "#     for row in reader:  \n",
    "#         if any(field.strip() for field in row.values()):  # Check for non-blank rows  \n",
    "#             data.append(row)\n",
    "  \n",
    "# # Write the dictionary to a JSON file  \n",
    "# with open(outputJson, mode='w', encoding='utf-8') as jsonFile:  \n",
    "#     json.dump(data, jsonFile, indent=4)  \n",
    "  \n",
    "# print(f'JSON file has been created: {outputJson}')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Invoke the Document Intelligence - Invoice Pre-built Model including the Key-Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from requests import get, post\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import re\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceJsonPlaceHolders(json, values):\n",
    "  # find all placeholders\n",
    "  placeholders = re.findall('<[\\w ]+>', json)\n",
    "  assert len(placeholders) == len(values), \"Please enter the values of all placeholders.\"\n",
    "\n",
    "  # replaces all placeholders with values\n",
    "  for k, v in values.items():\n",
    "      placeholder = \"<%s>\" % k\n",
    "      json = json.replace(placeholder, v)\n",
    "\n",
    "  return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceJsonPlaceHolder(json, values):\n",
    "  # replaces all placeholders with values\n",
    "  for k, v in values.items():\n",
    "      placeholder = \"<%s>\" % k\n",
    "      json = json.replace(placeholder, str(v))\n",
    "\n",
    "  return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime.date):\n",
    "            return obj.isoformat()\n",
    "        return super().default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDocs = []\n",
    "# sampleDocs = [\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_451102775.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_451102776.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_458680339.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_458680340.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_462460604.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_462460605.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_467091874.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_467091875.pdf',\n",
    "#     './Data/Invoice/4000835968/CORPADV/CORPADV_467091876.pdf',\n",
    "# ]\n",
    "# sampleDocs = [\n",
    "#     '../Data/Loan/4000835968/Python/CORPADV_451102775.pdf',\n",
    "# ]\n",
    "sampleOutputDocs = []\n",
    "if not os.path.exists(dataDirectory + CorpAdv):\n",
    "    sourcePath = dataDirectory\n",
    "else:\n",
    "    sourcePath = dataDirectory + \"CORPADV/\"\n",
    "\n",
    "if not os.path.exists(dataDirectory + Invoice):\n",
    "    sourcePathInv = dataDirectory\n",
    "else:\n",
    "    sourcePathInv = dataDirectory + \"INVOICE/\"\n",
    "    \n",
    "destinationPath = dataDirectory + \"Python/\"\n",
    "for file in os.listdir(sourcePath):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        sampleDocs.append(sourcePath + file)\n",
    "\n",
    "for file in os.listdir(sourcePathInv):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        sampleDocs.append(sourcePathInv + file)\n",
    "\n",
    "for file in os.listdir(destinationPath):\n",
    "    if file.endswith(\".json\"):\n",
    "        sampleOutputDocs.append(destinationPath + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docIntelligenceEndPoint = os.getenv('FormRecognizerEndPoint')\n",
    "docIntelligenceKey = os.getenv('FormRecognizerKey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeInvoice(pathAndFile):\n",
    "    postUrl = docIntelligenceEndPoint + \"documentintelligence/documentModels/prebuilt-invoice:analyze?api-version=2023-10-31-preview\"\n",
    "    postUrl = postUrl + \"&stringIndexType=utf16CodeUnit&pages=1&queryFields=Loan&features=keyValuePairs%2CqueryFields\"\n",
    "\n",
    "    #print(postUrl)\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        'Ocp-Apim-Subscription-Key': docIntelligenceKey\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"includeTextDetails\": True,\n",
    "        \"pages\" : 1,\n",
    "        \"features\":[\"keyValuePairs\",\"queryFields\"]\n",
    "\n",
    "    }\n",
    "\n",
    "    with open(pathAndFile, \"rb\") as f:\n",
    "        dataBytes = f.read()\n",
    "\n",
    "    try:\n",
    "        response = post(url=postUrl, data=dataBytes, headers=headers)\n",
    "        if response.status_code != 202:\n",
    "            print(\"POST Analyze failed\")\n",
    "            return None\n",
    "        #print(\"POST analyze succedded\", response.headers[\"Operation-Location\"])\n",
    "        getUrl = response.headers['Operation-Location']\n",
    "        #print(getUrl)\n",
    "    except Exception as e:\n",
    "        print(\"POST analyzed failed\" + str(e))\n",
    "        return None\n",
    "    \n",
    "    nTries = 50\n",
    "    nTry = 0\n",
    "    waitSec = 6\n",
    "\n",
    "    while nTry < nTries:\n",
    "        try:\n",
    "            getResponse  = get(url=getUrl, headers=headers)\n",
    "            respJson = json.loads(getResponse.text)\n",
    "            if (getResponse.status_code != 200):\n",
    "                print(\"Invoice Get Failed\")\n",
    "                return None\n",
    "            status = respJson[\"status\"]\n",
    "            #print(status)\n",
    "            if status == \"succeeded\":\n",
    "                fileName = os.path.basename(pathAndFile).replace(\".pdf\", \".json\")\n",
    "                with open(destinationPath + fileName, \"w\") as f:\n",
    "                    json.dump(respJson, f, indent=4, default=str)\n",
    "                return respJson\n",
    "            if status == \"failed\":\n",
    "                print(\"Analysis Failed\")\n",
    "                return None\n",
    "            time.sleep(waitSec)\n",
    "            nTry += 1\n",
    "        except Exception as e:\n",
    "            print(\"Exception during GET\" + str(e))\n",
    "            return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "def processAnalyzeResult(analyzeResults, rowId):\n",
    "    paymentDate = ''\n",
    "    serviceDate = ''\n",
    "    invoiceNbr = ''\n",
    "    vendorName = ''\n",
    "    expenseDesc = ''\n",
    "    amountPaid = ''\n",
    "    notes = ''\n",
    "    loanNumber = ''\n",
    "    invoiceTotal = ''\n",
    "    invoiceDate = ''\n",
    "    filledItems = []\n",
    "    #templateStructure = '{\"Payment Date\":\"<Payment_Date>\",\"Service Date\":\"<Service_Date>\", \"Expense Type\": \"\",\"Additional Expense Comments\":\"<Additional_Expense_Comments>\",\"Expense Description\": \"<Expense_Description>\",\"Amount Paid\": \"<Amount_Paid>\",\"Amount Claimed\": \"\",\"Amount Not Claimed\": \"\",\"Unclaimed Amount Reason\": \"\",\"Vendor Name\": \"<Vendor_Name>\",\"Invoice Number\": \"<Invoice_Number>\",\"Fee Type Code\": \"\",\"Recovery Type\": \"\",\"Actual Recovery Code\": \"\",\"Expense Code\": \"\",\"Fee Reference Comments\": \"\",\"File Name\": \"\",\"Document Available\": \"\",\"Notes\": \"<Notes>\"}'\n",
    "    templateStructure = '{\"Row_Id\":\"<Row_Id>\", \"Payment_Date\":\"<Payment_Date>\",\"Service_Date\":\"<Service_Date>\", \"Loan_Number\": \"<Loan_Number>\", \"Item_Description\": \"<Item_Description>\",\"Item_Amount\": \"<Item_Amount>\",\"Invoice_Nbr\": \"<Invoice_Nbr>\", \"Invoice_Date\": \"<Invoice_Date>\", \"Invoice_Total\": \"<Invoice_Total>\"}'\n",
    "    for idx, invoice in enumerate(analyzeResults[\"documents\"]):\n",
    "        for name, field in invoice[\"fields\"].items():\n",
    "            if name != \"Items\":\n",
    "                if name == \"VendorName\":\n",
    "                    vendorName = field[\"content\"]\n",
    "                if name == \"InvoiceId\":\n",
    "                    invoiceNbr = field[\"content\"]\n",
    "                if name == \"InvoiceDate\" and field[\"type\"] == \"date\":\n",
    "                    try:\n",
    "                        invoiceDate = field[\"valueDate\"]\n",
    "                    except:\n",
    "                        invoiceDate = field[\"content\"]\n",
    "                if name == \"InvoiceTotal\" and field[\"type\"] == \"currency\":\n",
    "                    try:\n",
    "                        invoiceTotal = field[\"valueCurrency\"][\"amount\"]\n",
    "                    except:\n",
    "                        invoiceTotal = field[\"content\"]\n",
    "                if name == \"CustomerAddress\":\n",
    "                    notes = field[\"content\"]\n",
    "                if name == \"Loan\":\n",
    "                    try:\n",
    "                        loanNumber = field[\"content\"]\n",
    "                    except:\n",
    "                        loanNumber = ''\n",
    "\n",
    "                #print(\"...{}: {} has confidence {}\".format(name, field.content, field.confidence))\n",
    "\n",
    "        for idx, item in enumerate(invoice[\"fields\"].get(\"Items\").get(\"valueArray\")):\n",
    "            #print(\"...Item #{}\".format(idx))\n",
    "            for name, field in item[\"valueObject\"].items():\n",
    "                if name == \"Amount\" and field[\"type\"] == \"currency\":\n",
    "                    try:\n",
    "                        amountPaid = field[\"valueCurrency\"][\"amount\"]\n",
    "                    except:\n",
    "                        amountPaid = field[\"content\"]\n",
    "                if name == \"Date\" and field[\"type\"] == \"date\":\n",
    "                    try:\n",
    "                        serviceDate = field[\"valueDate\"]\n",
    "                    except:\n",
    "                        serviceDate = field[\"content\"]\n",
    "                    #print(serviceDate)\n",
    "                if name == \"Description\":\n",
    "                    expenseDesc = field[\"content\"]\n",
    "\n",
    "            values = {'Row_Id':rowId, 'Payment_Date':paymentDate,'Service_Date': serviceDate, 'Loan_Number': loanNumber, \n",
    "                'Item_Description': expenseDesc, 'Item_Amount': Decimal(str(amountPaid)), 'Invoice_Nbr': invoiceNbr.removeprefix(\"60\"),\n",
    "                'Invoice_Date': invoiceDate, 'Invoice_Total': invoiceTotal}\n",
    "            filledItem = replaceJsonPlaceHolder(templateStructure,values)\n",
    "            filledItems.append(filledItem)\n",
    "            rowId += 1\n",
    "\n",
    "                #print(\"......{}: {} has confidence {}\".format(name, field.content, field.confidence))\n",
    "\n",
    "    # for idx, kv in enumerate(analyzeResults[\"keyValuePairs\"]):\n",
    "    #     if (kv[\"key\"] != None):\n",
    "    #         #if (kv[\"key\"][\"content\"] and kv[\"value\"][\"content\"]):\n",
    "    #         if (kv[\"key\"][\"content\"]):\n",
    "    #             if kv[\"key\"][\"content\"] == \"PaymentDate\":\n",
    "    #                 paymentDate = kv[\"value\"][\"content\"]\n",
    "    #             #print(\"Key...{}: Value...{}\".format(kv.key.content, kv.value.content))\n",
    "    \n",
    "\n",
    "    #print(values)\n",
    "    return filledItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_451102775.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_451102776.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_458680339.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_458680340.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_462460604.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_462460605.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_467091874.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_467091875.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_467091876.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_474710375.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_474710376.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_479097774.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_479097775.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_484308215.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_484308216.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_486056249.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_486056251.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_492723949.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_492723950.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_500536875.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_502195828.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_502195829.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_504206173.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_504206174.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_505378043.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_505378045.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_505378055.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_505378057.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_505378060.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_505378062.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_505378064.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_506419980.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_506787121.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_506787122.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_509935061.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_509935062.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_510225417.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_510225419.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_511632310.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_519915232.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_519915233.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_521612242.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_521612243.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_521612244.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_521612245.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_521612246.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_521612247.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_521619247.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_525562809.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_525562810.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_528143112.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/CORPADV/CORPADV_528143113.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/INVOICE/INVOICE_453076359.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/INVOICE/INVOICE_453076365.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/INVOICE/INVOICE_453076367.pdf\n",
      "--------Process Already analyzed Invoice:  ../Data/Loan/4000835968/INVOICE/INVOICE_453076374.pdf\n"
     ]
    }
   ],
   "source": [
    "extractedData = []\n",
    "\n",
    "docAnalysisClient = DocumentAnalysisClient(\n",
    "        endpoint=docIntelligenceEndPoint, credential=AzureKeyCredential(docIntelligenceKey))\n",
    "\n",
    "rowId = 0\n",
    "#output = {\"Advances\": []}\n",
    "for sampleDoc in sampleDocs:\n",
    "    fileName = os.path.basename(sampleDoc).replace(\".pdf\", \".json\")\n",
    "    # Check if we already have ran the analysis\n",
    "    #if os.path.exists(sampleDoc.replace(\".pdf\", \".json\")):\n",
    "    if os.path.exists(destinationPath + fileName):\n",
    "        print(\"--------Process Already analyzed Invoice: \", sampleDoc)\n",
    "        #with open(sampleDoc.replace(\".pdf\", \".json\"), \"r\") as f:\n",
    "        with open(destinationPath + fileName, \"r\") as f:\n",
    "            invoices = json.load(f)\n",
    "        analyzeResults = invoices['analyzeResult']\n",
    "        filledItems = processAnalyzeResult(analyzeResults, rowId)\n",
    "        for filledItem in filledItems:\n",
    "            rowId += 1\n",
    "            extractedData.append(ast.literal_eval(json.dumps(filledItem)))\n",
    "        continue\n",
    "    else:\n",
    "        print(\"--------Process Invoice: \", sampleDoc)\n",
    "        # with open(sampleDoc, \"rb\") as f:\n",
    "        #     poller = docAnalysisClient.begin_analyze_document(\n",
    "        #         \"prebuilt-invoice\", document=f, locale=\"en-US\",\n",
    "        #         pages=\"1\", features=[\"keyValuePairs\"]\n",
    "        #     )\n",
    "        # invoices = poller.result()\n",
    "        # with open(destinationPath + fileName, \"w\") as f:\n",
    "        #     json.dump(invoices.to_dict(), f, indent=4, default=str)\n",
    "        analyzeInvoice(sampleDoc)\n",
    "        with open(destinationPath + fileName, \"r\") as f:\n",
    "            invoices = json.load(f)\n",
    "        analyzeResults = invoices['analyzeResult']\n",
    "        filledItems = processAnalyzeResult(analyzeResults, rowId)\n",
    "        for filledItem in filledItems:\n",
    "            rowId += 1\n",
    "            extractedData.append(ast.literal_eval(json.dumps(filledItem)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updatedOutput = json.dumps(extractedData, indent=4, ensure_ascii=False)\n",
    "updatedOutput = updatedOutput.replace(\"\\\"{\", \"{\")\n",
    "updatedOutput = updatedOutput.replace(\"}\\\"\", \"}\")\n",
    "updatedOutput = updatedOutput.replace(\"\\\\\", \"\")\n",
    "tmpDf = pd.read_json(updatedOutput)\n",
    "tmpDf['Invoice_Total'] = pd.to_numeric(tmpDf['Invoice_Total'])\n",
    "tmpDf = tmpDf.sort_values(['Invoice_Nbr', 'Invoice_Date', \"Item_Amount\", 'Service_Date'], ascending=False)\n",
    "dropDuplicate = tmpDf.drop_duplicates(['Item_Amount','Invoice_Date','Invoice_Nbr'], keep='first')\n",
    "processedOutputJson = dataDirectory + \"Python/\" + loanNumber + \"_FrOut.json\"\n",
    "processedOutputFullJson = dataDirectory + \"Python/\" + loanNumber + \"_FullFrOut.json\"\n",
    "json.dumps(dropDuplicate.to_json(processedOutputJson, orient='records'), indent=2)\n",
    "json.dumps(tmpDf.to_json(processedOutputFullJson, orient='records'), indent=2)\n",
    "# Optionally, you can save this JSON data to a file  \n",
    "#with open(processedOutputJson, 'w') as json_file: \n",
    "#    json_file.write(updatedOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3A - Compare the Ground Truth JSON with the Document Intelligence JSON Output - Using \"Python\" Code\n",
    "##### Ensure that the output JSON is in the same format as the Ground Truth JSON for both Matching and Non-Matching Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and parse a JSON file  \n",
    "def loadJson(file_path):\n",
    "    with open(file_path, 'r') as file:  \n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare two lists of dictionaries based on key fields  \n",
    "def compareJsonArray(jsonArray1, jsonArray2, keyFields):  \n",
    "    #matchingOutput = {\"Advances\": []}\n",
    "    #nonMatchingOutput = {\"Advances\": []}\n",
    "    matchingObjects = []  \n",
    "    nonMatchingObjects = []  \n",
    "  \n",
    "    # Convert the second JSON array to a dictionary for faster lookup  \n",
    "    json_dict2 = {tuple(item[key] for key in keyFields): item for item in jsonArray2}\n",
    "    json_dict1 = {tuple(item[key] for key in keyFields): item for item in jsonArray1}\n",
    "  \n",
    "    # Iterate through the first JSON array and compare  \n",
    "    for item1 in jsonArray1:  \n",
    "        key = tuple(item1[key] for key in keyFields)  \n",
    "        item2 = json_dict2.get(key)  \n",
    "        if item2:  \n",
    "            # If a matching object is found based on key fields, store it  \n",
    "            #matchingObjects.append({'object1': item1, 'object2': item2})\n",
    "            item1.update({'Row_Id_Invoice': item2['Row_Id'], 'Invoice_Date': item2['Invoice_Date'], 'Invoice_Total': item2['Invoice_Total'],\n",
    "                          'Invoice_Item_Description': item2['Item_Description']})\n",
    "            matchingObjects.append(item1)\n",
    "        else:  \n",
    "            # If no matching object is found, store the non-matching object from the first array  \n",
    "            nonMatchingObjects.append(item1)  \n",
    "  \n",
    "    # Also check for any objects in the second array that didn't match any in the first  \n",
    "    for item2 in jsonArray2:  \n",
    "        key = tuple(item2[key] for key in keyFields)  \n",
    "        if key not in json_dict1:  \n",
    "            nonMatchingObjects.append(item2)  \n",
    "  \n",
    "    #matchingOutput[\"Advances\"] = matchingObjects\n",
    "    #nonMatchingOutput[\"Advances\"] = nonMatchingObjects\n",
    "    #return matchingOutput, nonMatchingOutput  \n",
    "    return matchingObjects, nonMatchingObjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonArray1 = loadJson(dataDirectory + \"Python/\" + loanNumber + \".json\")\n",
    "jsonArray2 = loadJson(dataDirectory + \"Python/\" + loanNumber + \"_FrOut.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the total amount of the invoices line item & the total amount of the ground truth and compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19421, 20996)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundTruthAmount = sum(map(lambda x: int(x['Item_Amount']), jsonArray1))\n",
    "scannedInvoiceAmount = sum(map(lambda x: int(x['Item_Amount']), jsonArray2))\n",
    "groundTruthAmount, scannedInvoiceAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching, nonMatching = compareJsonArray(jsonArray1['Advances'], jsonArray2['Advances'], \n",
    "#                  ['Invoice_Nbr', \"Service_Date\"])\n",
    "matching, nonMatching = compareJsonArray(jsonArray1, jsonArray2, \n",
    "                 ['Invoice_Nbr', \"Service_Date\", \"Item_Amount\"])\n",
    "# Output the results  \n",
    "#print(\"Matching Objects:\")  \n",
    "#print(json.dumps(matching, indent=4))  \n",
    "#print(\"\\nNon-Matching Objects:\")  \n",
    "#print(json.dumps(nonMatching, indent=4))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Incase if required to find duplicates\n",
    "\n",
    "# import hashlib\n",
    "\n",
    "# matchingFiltered = []\n",
    "# md5List = []\n",
    "\n",
    "# for item in matching[\"Advances\"]:\n",
    "#     md5Result = hashlib.md5(json.dumps(item, separators=(',', ':')).encode(\"utf-8\")).hexdigest()\n",
    "#     if md5Result not in md5List:\n",
    "#         md5List.append(md5Result)\n",
    "#         matchingFiltered.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchingOutputJson = dataDirectory + \"Python/\" + loanNumber + \"_FuzzyMatching.json\"\n",
    "nonMatchingOutputJson = dataDirectory + \"Python/\" + loanNumber + \"_FuzzyNonMatching.json\"\n",
    "# Optionally, you can save this JSON data to a file  \n",
    "with open(matchingOutputJson, 'w') as json_file:\n",
    "    json_file.write(json.dumps(matching, indent=4))\n",
    "\n",
    "with open(nonMatchingOutputJson, 'w') as json_file:\n",
    "    json_file.write(json.dumps(nonMatching, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Data from from Excel Output and Document Intelligence API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "excelDf = pd.read_json(dataDirectory + \"Python/\" + loanNumber + \".json\")\n",
    "frOutDf = pd.read_json(dataDirectory + \"Python/\" + loanNumber + \"_FrOut.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3b - Invoke LLM and build Custom Prompt that can be used to Compare the Ground Truth JSON with the Document Intelligence JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncateToken(string: str, encoding_name: str, max_length: int = 128000) -> str:\n",
    "    \"\"\"Truncates a text string based on max number of tokens.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    num_tokens = len(encoded_string)\n",
    "\n",
    "    if num_tokens > max_length:\n",
    "        string = encoding.decode(encoded_string[:max_length])\n",
    "\n",
    "    return string\n",
    "\n",
    "def getMessagesFromHistory(systemPrompt: str, userConv: str):\n",
    "        #messageBuilder = MessageBuilder(systemPrompt, modelId)\n",
    "        messages = []\n",
    "        messages.append({'role': 'system', 'content': systemPrompt})\n",
    "        userContent = truncateToken(string=userConv, encoding_name=\"cl100k_base\", max_length=128000)\n",
    "        messages.append({'role': \"user\", 'content': userContent})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# systemTemplate = \"\"\"Compare the provided JSON data arrays and determine the matching and non-matching rows based on key fields. Display the JSON output for both the matching and non-matching objects.\n",
    "#                     Context:\n",
    "#                     You have been given two JSON data arrays, Array A and Array B. Your task is to compare these arrays and identify the rows that match and do not match based on key fields. \n",
    "#                     The key fields in the JSON data arrays are \"Invoice Number\", \"Service Date\".\n",
    "                    \n",
    "#                     Instructions:\n",
    "#                     Compare Array A and Array B based on specified key fields.\n",
    "#                     Identify and display the matching rows in JSON format.\n",
    "#                     Identify and display the non-matching rows in JSON format.\n",
    "#                     Ensure that the output clearly indicates whether a row is matching or non-matching.\n",
    "                    \n",
    "#                     Do not include any explanations, only provide a  RFC8259 compliant JSON response following this format without deviation.\n",
    "#                     [{\n",
    "#                         \"Payment Date\": \"Payment_Date\",\n",
    "#                         \"Service Date\": \"2022-02-10\",\n",
    "#                         \"Expense Type\": \"Expense_Type\",\n",
    "#                         \"Additional Expense Comments\": \"Comments here\",\n",
    "#                         \"Expense Description\": \"Description here\",\n",
    "#                         \"Amount Paid\": \"Amount here\",\n",
    "#                         \"Amount Claimed\": \"Amount Claimed here\",\n",
    "#                         \"Amount Not Claimed\": \"Amount Not Claimed here\",\n",
    "#                         \"Unclaimed Amount Reason\": \"\",\n",
    "#                         \"Vendor Name\": \"Vendor Name here\",\n",
    "#                         \"Invoice Number\": \"Invoice Number here\",\n",
    "#                         \"Fee Type Code\": \"\",\n",
    "#                         \"Recovery Type\": \"\",\n",
    "#                         \"Actual Recovery Code\": \"\",\n",
    "#                         \"Expense Code\": \"\",\n",
    "#                         \"Fee Reference Comments\": \"\",\n",
    "#                         \"File Name\": \"\",\n",
    "#                         \"Document Available\": \"\",\n",
    "#                         \"Notes\": \"Notes here\"\n",
    "#                         }]\n",
    "#                         \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the LLM to compare two JSON data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemTemplate = \"\"\"Compare the provided JSON data arrays and determine the matching rows based on key fields. Display the JSON output for the matching objects.\n",
    "                    Context:\n",
    "                    You have been given two JSON data arrays, Array A and Array B. Your task is to compare these arrays and identify the rows that match and do not match based on key fields. \n",
    "                    The key fields in the JSON data arrays are \"Invoice Number\", \"Service Date\".\n",
    "                    \n",
    "                    Instructions:\n",
    "                    Compare Array A and Array B based on specified key fields.\n",
    "                    Identify and display the matching rows in JSON format.\n",
    "\n",
    "                    Array A is in the following format.\n",
    "                    [{\n",
    "                            \"Payment_Date\": \"Payment_Date\",\n",
    "                            \"Service_Date\": \"2022-02-10\",\n",
    "                            \"Invoice_Nbr\": \"Invoice Number here\",\n",
    "                            \"Item_Description\": \"\",\n",
    "                            \"Item_Amount\": \"\"\n",
    "                    }]\n",
    "                    \n",
    "                    Array B is in the following format.\n",
    "                    [{\n",
    "                            \"Payment_Date\": \"Payment_Date\",\n",
    "                            \"Service_Date\": \"2022-02-10\",\n",
    "                            \"Loan_Number\": \"Invoice Number here\",\n",
    "                            \"Item_Description\": \"\",\n",
    "                            \"Item_Amount\": \"\",\n",
    "                            \"Invoice_Nbr\": \"\"\n",
    "                    }]\n",
    "\n",
    "                    Do not include any explanations, only provide a  RFC8259 compliant JSON response following this format without deviation.\n",
    "                    [{\n",
    "                            \"Payment_Date\": \"Payment_Date\",\n",
    "                            \"Service_Date\": \"2022-02-10\",\n",
    "                            \"Loan_Number\": \"Invoice Number here\",\n",
    "                            \"Item_Description\": \"\",\n",
    "                            \"Item_Amount\": \"\",\n",
    "                            \"Invoice_Nbr\": \"\"\n",
    "                    }]\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because of the limitation on 4096 Tokens on the completion, let's break down the steps into multiple chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = (len(jsonArray1) - 1) // 5 + 1\n",
    "for i in range(chunks):\n",
    "     processedOutputJson = dataDirectory + \"Python/\" + loanNumber + \"_LlmMatching\" + str(i) + \".json\"\n",
    "     if (os.path.exists(processedOutputJson)):\n",
    "          continue\n",
    "     #chunkedJsonArray1 = {\"Advances\": []}\n",
    "     batch = jsonArray1[i*5:(i+1)*5]\n",
    "     chunkedJsonArray1 = batch\n",
    "     #print(chunkedJsonArray1)\n",
    "     content = \"\"\"Array A:{arrayA}\n",
    "            Array B:{arrayB}\n",
    "            \"\"\"\n",
    "     content = content.format(arrayA=json.dumps(chunkedJsonArray1, indent=4), arrayB=json.dumps(jsonArray2, indent=4))\n",
    "\n",
    "     messages = []\n",
    "     messages.append({'role': 'system', 'content': systemTemplate})\n",
    "     #userContent = truncateToken(string=content, encoding_name=\"gpt-4-1106-preview\", max_length=75000)\n",
    "     userContent = content\n",
    "     messages.append({'role': \"user\", 'content': userContent})\n",
    "\n",
    "     completion = client.chat.completions.create(\n",
    "          model=os.getenv('OpenAiGpt4Turbo'), \n",
    "          messages=messages,\n",
    "          temperature=0,\n",
    "          top_p=0,\n",
    "          max_tokens=4096,\n",
    "          n=1)\n",
    "     answer = completion.choices[0].message.content\n",
    "     answer = re.sub('\\n', '', answer)\n",
    "     answer = re.sub('```json', '', answer)\n",
    "     answer = re.sub('```', '', answer)\n",
    "\n",
    "     llmAnswer = json.loads(answer)\n",
    "     # Optionally, you can save this JSON data to a file  \n",
    "     with open(processedOutputJson, 'w') as json_file:  \n",
    "          json_file.write(json.dumps(llmAnswer, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all our LlmMatching Outputs to a single file\n",
    "#llmMatchingOutput = {\"Advances\": []}\n",
    "llmMatchingOutput = []\n",
    "for i in range(chunks):\n",
    "     with open(dataDirectory + \"Python/\" + loanNumber + \"_LlmMatching\" + str(i) + \".json\", \"r\") as f:\n",
    "          llmMatchingOutput.extend(json.load(f))\n",
    "\n",
    "processedOutputJson = dataDirectory + \"Python/\" + loanNumber + \"_LlmMatching.json\"\n",
    "# Optionally, you can save this JSON data to a file  \n",
    "with open(processedOutputJson, 'w') as json_file:  \n",
    "    json_file.write(json.dumps(llmMatchingOutput, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llmMatchingArray = loadJson(dataDirectory + \"Python/\" + loanNumber + \"LlmMatching.json\")\n",
    "#fuzzyMatchingArray = loadJson(dataDirectory + \"Python/\" + loanNumber + \"_FuzzyMatching.json\")\n",
    "#fuzzyNonMatchingArray = loadJson(dataDirectory + \"Python/\" + loanNumber + \"_FuzzyNonMatching.json\")\n",
    "llmMatchingDf = pd.read_json(dataDirectory + \"Python/\" + loanNumber + \"_LlmMatching.json\")\n",
    "fuzzyMatchingDf = pd.read_json(dataDirectory + \"Python/\" + loanNumber + \"_FuzzyMatching.json\")\n",
    "fuzzyNonMatchingDf = pd.read_json(dataDirectory + \"Python/\" + loanNumber + \"_FuzzyNonMatching.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run LLM to find non-matching rows between two JSON data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the provided Tables and determine the matching rows based on key fields. Display the output for the matching objects.\n",
    "    \n",
    "# Context:\n",
    "# You have been given two table, Table 1 and Table 2. Your task is to compare these tables and identify the rows that do match based on key fields. \n",
    "# The key fields in the Tables \"Invoice Number\", \"Service Date\".\n",
    "\n",
    "# Instructions:\n",
    "# Compare Table 1 and Table 2 based on specified key fields.\n",
    "# Identify and display the matching rows from Table 1 and Table 2 and display the output with RowId from both tables\n",
    "\n",
    "# Table 1\n",
    "# RowId   Payment Date\tService Date\tInvoice Number\tItem Description\tItem Amount\n",
    "# 1   11/26/2021\t\tCENG0122R35\tPROPERTY INSPECTION FEE\t15.00\n",
    "# 2   12/28/2021\t\tCENG0122R35\tPROPERTY INSPECTION FEE\t15.00\n",
    "# 3   2/16/2022\t2/10/2022\t109661_00490\tPROPERTY INSPECTION FEE\t20.00\n",
    "# 4   3/23/2022\t3/17/2022\t111413_00184\tPROPERTY INSPECTION FEE\t20.00\n",
    "# 5   4/18/2022\t4/13/2022\t112727_00307\tPROPERTY INSPECTION FEE\t20.00\n",
    "# 6   5/17/2022\t5/12/2022\t114129_00367\tPROPERTY INSPECTION FEE\t20.00\n",
    "# 7   5/17/2022\t5/12/2022\t114129_00367\tPROP PRES - PHOTOS\t30.00\n",
    "# 8   7/6/2022\t7/1/2022\t116869_00457\tPROPERTY INSPECTION FEE\t20.00\n",
    "# 9   8/9/2022\t8/4/2022\t118626_00253\tPROPERTY INSPECTION FEE\t20.00\n",
    "\n",
    "# Table 2\n",
    "# RowId   Invoice Number\tLoan Number\tInvoice Date\tProperty Address\tItem Description\tItem/Service Date\tItem Price\tInvoice Total\n",
    "# 1   110784761\t4000835968\t11/20/2021\t3554 Branden Rd\tInspection Photos\t\t0\t15\n",
    "# 2   110784761\t4000835968\t11/20/2021\t3554 Branden Rd\tInspections\t\t15\t15\n",
    "# 3   110784761\t4000835968\t11/20/2021\t3554 Branden Rd\tInspection Photos\t\t0\t15\n",
    "# 4   110784761\t4000835968\t11/20/2021\t3554 Branden Rd\tInspections\t\t15\t15\n",
    "# 5   110784761-0325503954\t4000835968\t11/20/2021\t3554 Branden Rd\tProperty Services - Insp - Drive by Inspection\t11/19/2021\t15\t15\n",
    "# 6   111466643\t4000835968\t12/22/2021\t3554 Branden Rd\tInspection Photos\t\t0\t15\n",
    "# 7   111466643\t4000835968\t12/22/2021\t3554 Branden Rd\tInspections\t\t15\t15\n",
    "# 8   111466643-0326171164\t4000835968\t12/22/2021\t3554 Branden Rd\tProperty Services - Insp - Drive by Inspection\t12/22/2021\t15\t15\n",
    "# 9   60109661_00490\t4000835968\t2/11/2022\t3554 Branden Rd\tOccupancy Inspection\t2/10/2022\t20\t20\n",
    "# 10  60109661_00490\t4000835968\t2/11/2022\t3554 Branden Rd\tVerify Occupancy\t2/10/2022\t20\t20\n",
    "# 11  60111413_00184\t4000835968\t3/18/2022\t3554 Branden Rd\tOccupancy Inspection\t3/17/2022\t20\t20\n",
    "# 12  60111413_00184\t4000835968\t3/18/2022\t3554 Branden Rd\tForeclosure\t3/17/2022\t20\t20\n",
    "# 13  60112727_00307\t4000835968\t4/14/2022\t3554 Branden Rd\tOccupancy Inspection\t4/13/2022\t20\t20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemTemplate = \"\"\"Compare the provided JSON data arrays and determine the non-matching rows based on key fields. Display the JSON output for the non-matching objects.\n",
    "                    Context:\n",
    "                    You have been given two JSON data arrays, Array A and Array B. Your task is to compare these arrays and identify the rows that do not match and do not match based on key fields. \n",
    "                    The key fields in the JSON data arrays are \"Invoice Number\", \"Service Date\".\n",
    "                    \n",
    "                    Instructions:\n",
    "                    Compare Array A and Array B based on specified key fields.\n",
    "                    Identify and display the non-matching rows in JSON format.\n",
    "                    \n",
    "                    Do not include any explanations, only provide a  RFC8259 compliant JSON response following this format without deviation.\n",
    "                    {\n",
    "                        \"Advances\":[{\n",
    "                            \"Payment Date\": \"Payment_Date\",\n",
    "                            \"Service Date\": \"2022-02-10\",\n",
    "                            \"Expense Type\": \"Expense_Type\",\n",
    "                            \"Additional Expense Comments\": \"Comments here\",\n",
    "                            \"Expense Description\": \"Description here\",\n",
    "                            \"Amount Paid\": \"Amount here\",\n",
    "                            \"Amount Claimed\": \"Amount Claimed here\",\n",
    "                            \"Amount Not Claimed\": \"Amount Not Claimed here\",\n",
    "                            \"Unclaimed Amount Reason\": \"\",\n",
    "                            \"Vendor Name\": \"Vendor Name here\",\n",
    "                            \"Invoice Number\": \"Invoice Number here\",\n",
    "                            \"Fee Type Code\": \"\",\n",
    "                            \"Recovery Type\": \"\",\n",
    "                            \"Actual Recovery Code\": \"\",\n",
    "                            \"Expense Code\": \"\",\n",
    "                            \"Fee Reference Comments\": \"\",\n",
    "                            \"File Name\": \"\",\n",
    "                            \"Document Available\": \"\",\n",
    "                            \"Notes\": \"Notes here\"\n",
    "                        }]\n",
    "                    }\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Excel Data:  63\n",
      "Total FrData:  52\n",
      "Total FrMatching:  23\n",
      "Total FrNoMatching:  40\n",
      "Total LlmMatching:  29\n"
     ]
    }
   ],
   "source": [
    "excelData = dataDirectory + \"Python/\" + loanNumber + \".json\"\n",
    "FrData = dataDirectory + \"Python/\" + loanNumber + \"_FrOut.json\"\n",
    "FrMatching = dataDirectory + \"Python/\" + loanNumber + \"_FrMatching.json\"\n",
    "FrNoMatching = dataDirectory + \"Python/\" + loanNumber + \"_FrNonMatching.json\"\n",
    "LlmMatching = dataDirectory + \"Python/\" + loanNumber + \"_LlmMatching.json\"\n",
    "LlmNoMatching = dataDirectory + \"Python/\" + loanNumber + \"_LlmNonMatching.json\"\n",
    "\n",
    "with open(excelData, 'r') as file:\n",
    "    excelData = json.load(file)\n",
    "    print(\"Total Excel Data: \", len(excelData['Advances']))\n",
    "\n",
    "with open(FrData, 'r') as file:\n",
    "    FrData = json.load(file)\n",
    "    print(\"Total FrData: \", len(FrData['Advances']))\n",
    "\n",
    "with open(FrMatching, 'r') as file:\n",
    "    FrMatching = json.load(file)\n",
    "    print(\"Total FrMatching: \", len(FrMatching['Advances']))\n",
    "\n",
    "with open(FrNoMatching, 'r') as file:\n",
    "    FrNoMatching = json.load(file)\n",
    "    print(\"Total FrNoMatching: \", len(FrNoMatching['Advances']))\n",
    "\n",
    "with open(LlmMatching, 'r') as file:\n",
    "    LlmMatching = json.load(file)\n",
    "    print(\"Total LlmMatching: \", len(LlmMatching['Advances']))\n",
    "\n",
    "# with open(LlmNoMatching, 'r') as file:\n",
    "#     LlmNoMatching = json.load(file)\n",
    "#     print(\"Total LlmNoMatching: \", len(LlmNoMatching['Advances']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
